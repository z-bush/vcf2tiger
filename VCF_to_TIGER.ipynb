{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:49:30.843059Z",
     "start_time": "2024-06-10T20:49:30.840300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VCF functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86ffa4ea9227910a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_files_in_directory(directory=str):\n",
    "    # Create an empty list to store the filenames\n",
    "    file_list = []\n",
    "\n",
    "    # Use the os module to get a list of all the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check that the file is a regular file (not a directory or a special file)\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            # Add the filename to the list\n",
    "            file_list.append(directory+filename)\n",
    "\n",
    "    return sorted(file_list)\n",
    "\n",
    "def get_vcf(vcf_file):\n",
    "\n",
    "    #eventually store header and data lines separately\n",
    "    header = []\n",
    "    vcf_list = []\n",
    "    \n",
    "    with open(vcf_file, newline = '', errors='ignore') as variants:  #open vcf file, take as tab delimited, create huge list that is the vcf file\n",
    "        variant_reader = csv.reader(variants, delimiter='\\t')\n",
    "        for variant in variant_reader:\n",
    "                vcf_list.append(variant)\n",
    "\n",
    "    #list of ALL lines in vcf file\n",
    "    vcf_lines = []\n",
    "\n",
    "    for i in range(len(vcf_list)): #for every line in VCF file\n",
    "\n",
    "        if vcf_list[i][0][0] == \"#\": #if header lines -> header\n",
    "\n",
    "            header.append(str(vcf_list[i][0]))\n",
    "\n",
    "        elif vcf_list[i][0][0] != \"#\": #if data/variant lines -> vcf_lines\n",
    "\n",
    "            vcf_lines.append(vcf_list[i])\n",
    "\n",
    "    return vcf_lines, header\n",
    "\n",
    "\n",
    "def build_ref_df(ref_vcf): #build pandas dataframe from VCF of 'reference'/parental SNPs\n",
    "\n",
    "    snp_lines, snp_header = get_vcf(ref_vcf) #read in VCF, split header from data lines\n",
    "\n",
    "    snp_data = []\n",
    "\n",
    "    for i in range(len(snp_lines)):\n",
    "        df_row = [ snp_lines[i][0], int(snp_lines[i][1]), snp_lines[i][3] ,snp_lines[i][4].split(',')[0] ] #create data rows that are only chromosome, position, reference allele, and variant\n",
    "        snp_data.append(df_row)\n",
    "\n",
    "    ref_df = pd.DataFrame(snp_data, columns=[\"chromosome\", \"position\", \"reference\", \"variant\"]) # make dataframe with given column names\n",
    "\n",
    "    ref_df= ref_df.reset_index(drop=True)\n",
    "\n",
    "    return ref_df\n",
    "\n",
    "\n",
    "def build_GATK_snp_df(snp_vcf): #build pandas dataframe from VCF samples with GATK genotype calls\n",
    "\n",
    "    snp_lines, snp_header = get_vcf(snp_vcf) #create data rows that are only chromosome, position, reference allele, and variant\n",
    "\n",
    "    # our file convention is 'project_name'-'germ_cell_origin'-'sample_number' ... for example, BSP-OR-001 where 'OR' = oocyte-derived recombinant and sample = 001\n",
    "\n",
    "\n",
    "    #modify this to fit your file naming convention and check for consistency in later functions\n",
    "    gamete = snp_vcf.split(\"/\")[-1].split(\".\")[0].split(\"-\")[1] # per our file naming convention, get germ cell of origin\n",
    "\n",
    "    sample = snp_vcf.split(\"/\")[-1].split(\".\")[0].split(\"-\")[2] # per our file naming convention, get sample number\n",
    "\n",
    "    snp_data = []\n",
    "\n",
    "    for i in range(len(snp_lines)):\n",
    "\n",
    "        chrom, position, dot1, ref, alt, qual, dot2, info, annotations, annot_data = snp_lines[i] #split SNP data into variables\n",
    "\n",
    "        annotation_dict = dict(zip(annotations.split(':'), annot_data.split(':'))) # create dictionary for info tags and data\n",
    "\n",
    "        if alt == '<NON_REF>': #replace with more conventional '.' for lack of variant allele\n",
    "            alt = '.'\n",
    "\n",
    "        alt = alt.split(\",\")[0] #take most prevalent alternate allele\n",
    "\n",
    "        if annot_data == '0/0' or annot_data[-1] == './.' or 'DP' not in annotation_dict: \n",
    "        #if weird site with genotype called but NaN or no read counts given...USELESS...skip this line...\n",
    "        \n",
    "            continue\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            DP = int(annotation_dict['DP']) #get total read number used to call variants\n",
    "\n",
    "            GT = annotation_dict['GT'] #get preliminary genotype call\n",
    "\n",
    "            ref_reads = 0\n",
    "            var_reads = 0\n",
    "\n",
    "            if 'AD' in annotation_dict:\n",
    "                reads = annotation_dict['AD'].split(',') # if Allelic Depth field is present, split into reads per allele\n",
    "\n",
    "                ref_reads = int(reads[0]) #ref reads are always first\n",
    "\n",
    "                if len(reads)>1:\n",
    "                    var_reads = int(reads[1]) # if variant allele present, get read counts\n",
    "\n",
    "            elif 'AD' not in annotation_dict: #unless no variant alleles present, ref reads = total reads\n",
    "\n",
    "                ref_reads = DP\n",
    "\n",
    "\n",
    "\n",
    "            qual = 0 #qual will be zero unless variant is called\n",
    "\n",
    "            if snp_lines[i][4] != '.' and snp_lines[i][4] != '<NON_REF>':\n",
    "\n",
    "                qual = float(snp_lines[i][5]) #get QUAL score\n",
    "\n",
    "            df_row = [ chrom, int(position), ref, alt, qual, int(ref_reads), int(var_reads), DP, GT, gamete, sample] #make dataframe rows\n",
    "            snp_data.append(df_row)\n",
    "\n",
    "    #make dataframe with given column names matching data in line 116 (df_row)\n",
    "    snp_df = pd.DataFrame(snp_data, columns=[\"chromosome\", \"position\", \"reference\", \"variant\", 'QUAL',\n",
    "                                             'ref_reads', 'variant_reads', 'DP', 'genotype', 'gamete', 'sample_num'])\n",
    "\n",
    "\n",
    "    snp_df = snp_df[snp_df['genotype']!='./.'] #drop lines with no genotype called\n",
    "    snp_df= snp_df.reset_index(drop=True)\n",
    "    snp_df = snp_df.drop(columns='genotype')\n",
    "\n",
    "    #change names of chromosomes\n",
    "    # chrom_name_dict = {'I_1':\"N2_chrI\", 'II_1':\"N2_chrII\", 'III_1':\"N2_chrIII\", 'IV':\"N2_chrIV\", 'V_1':\"N2_chrV\", 'X_1':\"N2_chrX\"}\n",
    "    # snp_df = snp_df.replace(chrom_name_dict)\n",
    "\n",
    "    return snp_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:49:30.854888Z",
     "start_time": "2024-06-10T20:49:30.844328Z"
    }
   },
   "id": "273c43b8b28c5b6",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parental and Sample VCF dataframe merging"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e28f05d2e73669e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def merge_sample_ref_dfs(sample_vcf_df, ref_vcf_df):\n",
    "\n",
    "    ## merge vcf dataframes on [chromosome, position] to retain every position/marker in the reference marker set\n",
    "\n",
    "    df_merge = ref_vcf_df.merge(sample_vcf_df, on=['chromosome','position'], copy=False, how='outer')\n",
    "\n",
    "    ## drop positions/rows that are not in reference marker set\n",
    "\n",
    "    df_merge = df_merge.drop(df_merge.loc[df_merge['reference_x'].isna()].index, inplace=False)\n",
    "\n",
    "    ## drop redundant reference column\n",
    "\n",
    "    df_merge = df_merge.drop(columns=['reference_y'])\n",
    "\n",
    "    ## replace NaN with zeros for marker sites with no variant reads and not called in GATK vcf, set proper empty variant allele\n",
    "\n",
    "    df_merge[['QUAL', \"ref_reads\", \"variant_reads\", \"DP\"]] = df_merge[['QUAL', \"ref_reads\", \"variant_reads\", \"DP\"]].fillna(0)\n",
    "    df_merge['variant_y'] = df_merge['variant_y'].fillna('.')\n",
    "\n",
    "    ## refill missing gamete and sample number values that were NaN\n",
    "\n",
    "    gamete = df_merge[(df_merge['gamete']=='SR') | (df_merge['gamete']=='OR')].gamete.unique()[0]\n",
    "    sample_num = [x for x in df_merge.sample_num.unique() if isinstance(x, str)][0]\n",
    "\n",
    "    df_merge['gamete'] = gamete\n",
    "    df_merge['sample_num'] = sample_num\n",
    "\n",
    "    ## if sample variant is not target marker variant, change to empty variant allele and set variant reads to zero\n",
    "\n",
    "    df_merge['variant_y'] = df_merge.apply(lambda row: '.' if row['variant_x'] != row['variant_y'] else row['variant_y'], axis =1)\n",
    "    df_merge['variant_reads'] = df_merge.apply(lambda row: 0.0 if row['variant_x'] != row['variant_y'] else row['variant_reads'], axis =1)\n",
    "\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def build_master_dataframe_list(sample_vcf_files=list, ref_vcf_file=str, minimum_dp=2, minimum_var_reads=1, minimum_var_qual=30):\n",
    "\n",
    "    print('building dataframe from reference vcf...')\n",
    "    ref_df = build_ref_df(ref_vcf_file)\n",
    "\n",
    "    sample_vcf_merges = []\n",
    "\n",
    "    print('building dataframes from each sample vcf and merging with reference vcf...')\n",
    "\n",
    "    for i in range(len(sample_vcf_files)):\n",
    "\n",
    "        merge_df = merge_sample_ref_dfs( build_GATK_snp_df(sample_vcf_files[i]), ref_df)\n",
    "\n",
    "        sample_vcf_merges.append(merge_df)\n",
    "\n",
    "        print(sample_vcf_files[i].split('/')[-1], 'complete...')\n",
    "\n",
    "    print('building master dataframe of chromosomes...')\n",
    "    master_recombinant_df = pd.concat(sample_vcf_merges, ignore_index=True)\n",
    "    \n",
    "    ##uncomment below to make use of filters for minimum read depth, minimum variant reads, and minimum QUAL score\n",
    "\n",
    "#     print(\"filtering all sites: requiring minimum of\", minimum_dp, \"total reads per site...\")\n",
    "#     master_recombinant_df_filt1 = master_recombinant_df.drop(master_recombinant_df.loc[ (master_recombinant_df['DP']<minimum_dp) ].index, inplace=False)\n",
    "\n",
    "#     print('filtering variant SNPs: require a minimum of', minimum_var_reads, 'variant reads...')\n",
    "#     master_recombinant_df_filt2 = master_recombinant_df_filt1.drop(master_recombinant_df_filt1.loc[(master_recombinant_df_filt1['Code']=='CB4856') & (master_recombinant_df_filt1['variant_reads']<minimum_var_reads)].index, inplace=False)\n",
    "\n",
    "#     print('filtering variant SNPs: require a minimum QUAL score of ', minimum_var_qual, '...')\n",
    "#     master_recombinant_df_filt3 = master_recombinant_df_filt2.drop(master_recombinant_df_filt2.loc[(master_recombinant_df_filt2['Code']=='CB4856') & (master_recombinant_df_filt2['QUAL']<minimum_var_qual)].index, inplace=False)\n",
    "\n",
    "    #master_recombinant_df = master_recombinant_df_filt3\n",
    "\n",
    "    master_recombinant_df = master_recombinant_df.drop(columns=['variant_y'])\n",
    "    master_recombinant_df = master_recombinant_df.rename(columns={'reference_x':'reference', 'variant_x':'variant'})\n",
    "    master_recombinant_df = master_recombinant_df.astype({'ref_reads':int, 'variant_reads':int})\n",
    "    master_recombinant_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"done!\")\n",
    "\n",
    "    return master_recombinant_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:49:30.864911Z",
     "start_time": "2024-06-10T20:49:30.857530Z"
    }
   },
   "id": "45a94de0bea412d8",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TIGER functions\n",
    "\n",
    "* adapted methods from Rowan et. al. : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349092/\n",
    "* original TIGER scripts found at: https://github.com/betharowan/TIGER_Scripts-for-distribution\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a959d3281094bc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_TIGER_inputs_and_mkdirs(new_tiger_directory=str):\n",
    "\n",
    "    os.mkdir(new_tiger_directory)\n",
    "\n",
    "    # Create an empty list to store the filenames\n",
    "    file_list = []\n",
    "\n",
    "    # Use the os module to get a list of all the files in the current directory\n",
    "\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "\n",
    "        if os.path.isfile(os.path.join(os.getcwd(), filename)):\n",
    "\n",
    "            if filename.split('.')[-2] == \"tiger_input\":\n",
    "\n",
    "                os.mkdir(new_tiger_directory+'/'+filename.split('.')[0])\n",
    "\n",
    "                os.replace( os.getcwd()+'/'+filename , new_tiger_directory+'/'+filename.split('.')[0]+'/'+filename )\n",
    "\n",
    "def create_TIGER_inputs_and_directory(master_dataframe, new_project_dir=str):\n",
    "\n",
    "    if new_project_dir not in os.listdir():\n",
    "\n",
    "        for gamete in master_dataframe.gamete.unique():\n",
    "\n",
    "            for sample_num in master_dataframe.sample_num.unique(): #subset master by individual samples...\n",
    "    \n",
    "                filename = str(gamete) + \"_\" + str(sample_num)+\".tiger_input.txt\"\n",
    "    \n",
    "                df = master_dataframe[(master_dataframe['gamete']==gamete) & (master_dataframe['sample_num']==sample_num)]\n",
    "    \n",
    "    \n",
    "                df['tiger_chrom'] = df.apply(lambda row: tiger_chrom_name_dict[row['chromosome']], axis=1)\n",
    "    \n",
    "                df.to_csv(filename, sep=\"\\t\", header=False, index=False, columns = ['tiger_chrom', 'position', 'reference', 'ref_reads', 'variant', 'variant_reads'])\n",
    "    \n",
    "            get_TIGER_inputs_and_mkdirs(new_tiger_directory=new_project_dir)\n",
    "    \n",
    "        else:\n",
    "            print(\"No TIGER input or folders made. This project folder already exists...\")\n",
    "\n",
    "def run_TIGER_pipeline(master_dataframe, project_dir=str):\n",
    "\n",
    "    #try to make TIGER inputs\n",
    "\n",
    "\n",
    "    if project_dir not in os.listdir(): # if project folder not made...\n",
    "\n",
    "        print('making TIGER inputs...')\n",
    "        create_TIGER_inputs_and_directory(master_dataframe, new_project_dir=project_dir) #make inputs, project directory, and sample sub-directories\n",
    "\n",
    "        for sample_dir in os.listdir(project_dir):\n",
    "\n",
    "            sample_folder = os.getcwd()+'/'+project_dir+'/'+sample_dir+'/'\n",
    "\n",
    "            if sample_dir != '.DS_Store':\n",
    "\n",
    "                if sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' not in os.listdir(sample_folder): #if folder has input but missing this TIGER output, do TIGER\n",
    "\n",
    "                    subprocess.run(['sh', 'run_TIGER.sh', os.getcwd()+'/'+project_dir+'/'+sample_dir+'/', sample_dir])\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' in os.listdir(sample_folder): #if folder has input and has this TIGER output, skip...\n",
    "                    print(sample_dir, 'already processed...')\n",
    "                    continue\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' not in os.listdir(sample_folder): #warn if input not in folder for some reason\n",
    "                    print(sample_dir, 'needs TIGER input...')\n",
    "\n",
    "\n",
    "\n",
    "    elif project_dir in os.listdir(): #if project directory alredy exists\n",
    "        for sample_dir in os.listdir(project_dir): #for every sample directory, run tiger\n",
    "\n",
    "            if sample_dir != '.DS_Store': #skip mac's DS_store file...useless\n",
    "\n",
    "                sample_folder = os.getcwd()+'/'+project_dir+'/'+sample_dir+'/'\n",
    "\n",
    "                if sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' not in os.listdir(sample_folder): #if folder has input but missing this TIGER output, do TIGER\n",
    "\n",
    "                    subprocess.run(['sh', 'run_TIGER.sh', os.getcwd()+'/'+project_dir+'/'+sample_dir+'/', sample_dir])\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' in os.listdir(sample_folder): #if folder has input and has this TIGER output, skip...\n",
    "                    print(sample_dir, 'already processed...')\n",
    "                    continue\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' not in os.listdir(sample_folder): #warn if input not in folder for some reason\n",
    "                    print(sample_dir, 'needs TIGER input...')\n",
    "                    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:49:30.876379Z",
     "start_time": "2024-06-10T20:49:30.866385Z"
    }
   },
   "id": "cef85ccfb7a33e48",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions for processing TIGER outputs and creating new dataframes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194f81d2ca56be6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing TIGER outputs to create SNP/marker dataframes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "201b54ce60fa2acb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_TIGER_files(project_dir=str, file_pattern=str):\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for folder in os.listdir(project_dir):\n",
    "\n",
    "        if folder[0] != '.':\n",
    "\n",
    "            files.append(project_dir+'/'+folder+\"/\"+folder+file_pattern)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def create_TIGER_master_df(project_dir=str, file_pattern=str):\n",
    "    #this dataframe this function makes is relatively large because it gives you genotype and HMM state calls for every SNP marker across each chromosome for each sample\n",
    "    \n",
    "    #This is closest to the 'raw' data that TIGER outputs, but this format is mostly useful for plotting and visualization of individual chromosomes for manual inspection of SNPs and crossover calls\n",
    "\n",
    "    #get list of file names for all TIGER outputs\n",
    "    files_list = get_TIGER_files(project_dir=project_dir, file_pattern=file_pattern)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files_list:\n",
    "\n",
    "        #make dataframe; adjust column names if you end up using different TIGER output files than I did (file_pattern='.CO_estimates.txt')\n",
    "        df = pd.read_csv(file, sep=\"\\t\", header=None, names = [\"Sample\", \"chromosome\", \"position\", \"base_geno\", \"hmm_state1\", \"hmm_state2\",\n",
    "                                                   \"reference\", \"ref_reads\", \"variant\", \"var_reads\"])\n",
    "\n",
    "        #This dict will convert parental arabidopsis genotype calls (Col and Ler) to C. elegans genotype calls (N2 is Bristol WT, CB4856 is Hawaiian WT)\n",
    "        #Replace these values with your preferred genotype labels as needed\n",
    "        genotype_dict = {'CC':ref_parental_genotype, \"CL\":\"het\", \"LL\":alt_parental_genotype, \"CU\":\"u\"+ref_parental_genotype, \"LU\":'u'+alt_parental_genotype, \"UN\":\"unknown\", '?':\"?\"}\n",
    "        df = df.replace(genotype_dict)\n",
    "        \n",
    "        #for our specific cross scheme to ID crossovers, heterozygous and homozygous CB4856 calls are should all be considered CB4856\n",
    "        df['hmm_state1'] = df['hmm_state1'].replace({'het':alt_parental_genotype})\n",
    "        \n",
    "        #add a unique identifier for each chromosome\n",
    "        #example... BSP-OR-001-1 = (project)-(gamete)-(sample number)-(chromosome)\n",
    "        df['chrom_id'] = df.apply(lambda row: row['Sample']+\"-\"+str(row['chromosome']), axis=1)\n",
    "\n",
    "        print(file, 'done')\n",
    "        data.append(df)\n",
    "\n",
    "    # make dataframe from all samples in data list\n",
    "    data = pd.concat(data)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:49:30.883509Z",
     "start_time": "2024-06-10T20:49:30.877694Z"
    }
   },
   "id": "243a082ada31d16",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing TIGER outputs to create intervals dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15065fdee8a03f3b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_transition_intervals_df(master_df):\n",
    "    \n",
    "    #These new transition intervals are the potential crossover intervals that can be classified later as crossover or 'not crossover'\n",
    "\n",
    "    new_dfs = []\n",
    "\n",
    "    for chrom_id in master_df.chrom_id.unique():\n",
    "\n",
    "        df = master_df[master_df['chrom_id']==chrom_id]\n",
    "\n",
    "        starts = list(df.start)\n",
    "        stops = list(df.stop)\n",
    "        states = list(df.hmm_state)\n",
    "\n",
    "        new_intervals = []\n",
    "\n",
    "        for i in range(len(stops)-1):\n",
    "\n",
    "            if states[i] != states[i+1]:\n",
    "\n",
    "                row = [ df['sample'].unique()[0], df['chromosome'].unique()[0], stops[i], starts[i+1], 'transition', df['chrom_id'].unique()[0], (starts[i+1]-stops[i]) ]\n",
    "\n",
    "                new_intervals.append(row)\n",
    "\n",
    "        new_intervals_df = pd.DataFrame(new_intervals, columns=df.columns)\n",
    "\n",
    "\n",
    "        full_intervals_df = pd.concat([df, new_intervals_df]).sort_values(by='start').reset_index(drop=True)\n",
    "\n",
    "        new_dfs.append(full_intervals_df)\n",
    "\n",
    "\n",
    "\n",
    "    return pd.concat(new_dfs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_state_intervals_df(project_dir=str, file_pattern=str):\n",
    "    #This will create a much smaller dataframe than the marker dataframe above by collapsing runs of the same HMM state into intervals and creating a new row for a 2-marker transition interval between HMM states\n",
    "\n",
    "    #for our specific cross scheme to ID crossovers, heterozygous and homozygous CB4856 calls are should all be considered CB4856\n",
    "    genotype_dict = {'CC':ref_parental_genotype, \"CL\":alt_parental_genotype, \"LL\":alt_parental_genotype, \"CU\":\"u\"+ref_parental_genotype, \"LU\":'u'+alt_parental_genotype, \"UN\":\"unknown\", '?':\"?\"}\n",
    "\n",
    "    files_list = get_TIGER_files(project_dir=project_dir, file_pattern=file_pattern)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files_list:\n",
    "\n",
    "        #create df given columns in files using file_pattern='.CO_estimates.breaks.txt' in TIGER outputs\n",
    "        df = pd.read_csv(file, sep=\"\\t\", header=None, names = [\"sample\", \"chromosome\", \"start\", \"stop\", \"hmm_state\"])\n",
    "\n",
    "        df['hmm_state'] = df.apply(lambda row: genotype_dict[(row['hmm_state'])], axis=1)\n",
    "        df['chrom_id'] = df.apply(lambda row: row['sample']+\"-\"+str(row['chromosome']), axis=1)\n",
    "        df['length'] = df.apply(lambda row: row['stop']-row['start']+1, axis=1)\n",
    "#         df['gamete'] = df['sample'].unique()[0].split(\"-\")[1]\n",
    "\n",
    "        data.append(df)\n",
    "\n",
    "\n",
    "    master_df = pd.concat(data)\n",
    "\n",
    "    master_df = make_transition_intervals_df(master_df)\n",
    "\n",
    "    master_df = master_df.sort_values(by=['chrom_id', 'chromosome', 'start']).reset_index(drop=True)\n",
    "\n",
    "    return master_df\n",
    "\n",
    "\n",
    "def get_marker_counts_per_interval(master_bases_df, master_intervals_df):\n",
    "    \n",
    "    #This is useful to retain marker density information for each interval, which is needed for later maths and classification of crossover intervals\n",
    "\n",
    "    new_dfs = []\n",
    "\n",
    "    for chrom_id in master_bases_df.chrom_id.unique():\n",
    "\n",
    "        bases_df = master_bases_df[master_bases_df['chrom_id']==chrom_id]\n",
    "        intervals_df = master_intervals_df[master_intervals_df['chrom_id']==chrom_id]\n",
    "\n",
    "        starts = list(intervals_df.start)\n",
    "        stops = list(intervals_df.stop)\n",
    "\n",
    "        counts = []\n",
    "\n",
    "        for i in range(len(starts)):\n",
    "\n",
    "            counts.append(len(bases_df[(bases_df['position']>=starts[i]) & (bases_df['position']<=stops[i]) ]))\n",
    "\n",
    "        intervals_df['marker_counts'] = counts\n",
    "\n",
    "        new_dfs.append(intervals_df)\n",
    "\n",
    "    markers_counted_df = pd.concat(new_dfs)\n",
    "\n",
    "    return markers_counted_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:49:30.893679Z",
     "start_time": "2024-06-10T20:49:30.884986Z"
    }
   },
   "id": "5d58cf85d0634ac1",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ubiquitious items and file names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec2b09f0d949f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['/Users/zac/Desktop/VCF_to_TIGER/sample_vcf/BSP-OR-004.vcf']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary of chromosome names and corresponding lengths, change these as needed!\n",
    "n2_fasta_names = ['N2_chrI', 'N2_chrII', 'N2_chrIII', 'N2_chrIV', 'N2_chrV', 'N2_chrX']\n",
    "n2_chrom_lengths = [15114068, 15311845, 13819453, 17493838, 20953657, 17739129]\n",
    "\n",
    "tiger_chrom_len_dict = dict(zip([1,2,3,4,5,6], n2_chrom_lengths))\n",
    "tiger_chrom_name_dict = dict(zip(n2_fasta_names, [1,2,3,4,5,6]))\n",
    "\n",
    "ref_parental_genotype = 'N2' #change this to your reference genotype\n",
    "alt_parental_genotype = 'CB4856' #change this to your alternate parental genotype\n",
    "\n",
    "sample_vcfs_path = '/Users/zac/Desktop/VCF_to_TIGER/sample_vcf/'\n",
    "\n",
    "cb_ref_vcf = '/Users/zac/Desktop/VCF_to_TIGER/reference_vcf/CB.aligned.to.N2.SNPs.noHet.no-repeats.vcf'\n",
    "\n",
    "sample_vcfs = get_files_in_directory(sample_vcfs_path)\n",
    "sample_vcfs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:50:05.128014Z",
     "start_time": "2024-06-10T20:50:05.123672Z"
    }
   },
   "id": "56270574772a5e9f",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Pipeline: VCF processing > TIGER HMM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97ba67cf857d117"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dataframe from reference vcf...\n",
      "building dataframes from each sample vcf and merging with reference vcf...\n",
      "BSP-OR-004.vcf complete...\n",
      "building master dataframe of chromosomes...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#VCF Processing and save\n",
    "\n",
    "vcf_master_df = build_master_dataframe_list(sample_vcf_files=sample_vcfs, ref_vcf_file=cb_ref_vcf)\n",
    "\n",
    "#Save the dataframe as a pickle file\n",
    "save_vcf_data_as = \"test_vcfs.pickle.gzip\"\n",
    "vcf_master_df.to_pickle(save_vcf_data_as, compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:50:59.304942Z",
     "start_time": "2024-06-10T20:50:05.156696Z"
    }
   },
   "id": "79c3341ad0df4f1c",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making TIGER inputs...\n",
      "No TIGER input or folders made. This project folder already exists...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: run_TIGER.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#run TIGER scripts to get HMM outputs\n",
    "run_TIGER_pipeline(vcf_master_df, project_dir='TIGER_test_output/')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:53:38.723578Z",
     "start_time": "2024-06-10T20:53:37.366770Z"
    }
   },
   "id": "ef9ceb2f2fc42d74",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIGER_test_output/OR_004/OR_004.CO_estimates.txt done\n"
     ]
    }
   ],
   "source": [
    "#generate TIGER marker and interval dataframes\n",
    "tiger_marker_df = create_TIGER_master_df(project_dir='TIGER_test_output', file_pattern='.CO_estimates.txt')\n",
    "tiger_marker_df.to_pickle('TIGER_hmm_states.all_markers.pickle.gzip', compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:51:05.109938Z",
     "start_time": "2024-06-10T20:50:59.311601Z"
    }
   },
   "id": "da2c8ed1995eadb0",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     sample chromosome    start      stop   hmm_state  chrom_id    length  \\\n0    OR_004          1        1   1095319      CB4856  OR_004-1   1095319   \n1    OR_004          1  1095319   1095379  transition  OR_004-1        60   \n2    OR_004          1  1095379   1109260          N2  OR_004-1     13882   \n3    OR_004          1  1109260   1109268  transition  OR_004-1         8   \n4    OR_004          1  1109268   1109645      CB4856  OR_004-1       378   \n..      ...        ...      ...       ...         ...       ...       ...   \n503  OR_004          6        1   1562534          N2  OR_004-6   1562534   \n504  OR_004          6  1562534   1562582  transition  OR_004-6        48   \n505  OR_004          6  1562582   1625215      CB4856  OR_004-6     62634   \n506  OR_004          6  1625215   1625326  transition  OR_004-6       111   \n507  OR_004          6  1625326  17739129          N2  OR_004-6  16113804   \n\n     marker_counts  \n0             2050  \n1                2  \n2              265  \n3                2  \n4               26  \n..             ...  \n503           1904  \n504              2  \n505             68  \n506              2  \n507          15412  \n\n[508 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample</th>\n      <th>chromosome</th>\n      <th>start</th>\n      <th>stop</th>\n      <th>hmm_state</th>\n      <th>chrom_id</th>\n      <th>length</th>\n      <th>marker_counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OR_004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1095319</td>\n      <td>CB4856</td>\n      <td>OR_004-1</td>\n      <td>1095319</td>\n      <td>2050</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OR_004</td>\n      <td>1</td>\n      <td>1095319</td>\n      <td>1095379</td>\n      <td>transition</td>\n      <td>OR_004-1</td>\n      <td>60</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>OR_004</td>\n      <td>1</td>\n      <td>1095379</td>\n      <td>1109260</td>\n      <td>N2</td>\n      <td>OR_004-1</td>\n      <td>13882</td>\n      <td>265</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>OR_004</td>\n      <td>1</td>\n      <td>1109260</td>\n      <td>1109268</td>\n      <td>transition</td>\n      <td>OR_004-1</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>OR_004</td>\n      <td>1</td>\n      <td>1109268</td>\n      <td>1109645</td>\n      <td>CB4856</td>\n      <td>OR_004-1</td>\n      <td>378</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>OR_004</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1562534</td>\n      <td>N2</td>\n      <td>OR_004-6</td>\n      <td>1562534</td>\n      <td>1904</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>OR_004</td>\n      <td>6</td>\n      <td>1562534</td>\n      <td>1562582</td>\n      <td>transition</td>\n      <td>OR_004-6</td>\n      <td>48</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>OR_004</td>\n      <td>6</td>\n      <td>1562582</td>\n      <td>1625215</td>\n      <td>CB4856</td>\n      <td>OR_004-6</td>\n      <td>62634</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>506</th>\n      <td>OR_004</td>\n      <td>6</td>\n      <td>1625215</td>\n      <td>1625326</td>\n      <td>transition</td>\n      <td>OR_004-6</td>\n      <td>111</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>507</th>\n      <td>OR_004</td>\n      <td>6</td>\n      <td>1625326</td>\n      <td>17739129</td>\n      <td>N2</td>\n      <td>OR_004-6</td>\n      <td>16113804</td>\n      <td>15412</td>\n    </tr>\n  </tbody>\n</table>\n<p>508 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger_pre_intervals = create_state_intervals_df(project_dir='TIGER_test_output', file_pattern='.CO_estimates.breaks.txt')\n",
    "tiger_intervals = get_marker_counts_per_interval(tiger_marker_df, tiger_pre_intervals)\n",
    "\n",
    "tiger_intervals.to_pickle('TIGER_hmm_intervals.pickle.gzip', compression='gzip')\n",
    "tiger_intervals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T20:51:05.483064Z",
     "start_time": "2024-06-10T20:51:05.111720Z"
    }
   },
   "id": "683a9a5f0d2914a5",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
